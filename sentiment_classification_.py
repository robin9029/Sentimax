# -*- coding: utf-8 -*-
"""Sentiment Classification .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l0CyDCiD3cpXYWibi1pjIk3cfJHyh0hh

challange on https://competitions.codalab.org/competitions/20654
SentiMix Hindi challange
"""

#In terms of mathematics lets see how model predcit :
we have given Training data, I have choosen Multinomial Naive bayes,
based on Training we predict Validation & Test set

1. Given train sample- 14000*1 
2. TfidfVectorizer converts train*max_features = 14000*14000 dimesnion ,where max_features choosen as= 14000 
3. Model Multinomial Niave bayes, 

lets say hypothesis function is  say H(x) = theta*X , 
Multinomial Model gives optimized_theta

theta := theta - 1/2*(m)*(alpha)* Partial deravitive of (Cost function)

where m= sample 
alpha= learning rate can be choosen , .001 or .00001

Cost function= mean square Error = 1/2*m* [h(x) - y]^2, 
where h(x) = theta*X   hypothesis function,
      y = actual result

theta choosen based on max_features,here theta = 14000*1 

4.  validation_set = 3000*1 matrix
 TfidfVectorizer converts validation_set = 3000*14000 matrix

predict =validation_set*optimized_theta: [3000*14000] * [14000*1] = 3000*1



lets Understand with one example:

a)let say we have tweet ,  tweet: aaj ka din is ok,
Now lets say, we have to predict sentiment of tweet into any of 3 Class : (positive ,negative,neutral) 

b)Predict(tweet) =(Optimized_theta*tweet)  , Optimized_theta= from Model

c)lets say based on training Model, Our Model has calculate Sentiment as Probability 

tweet:( aaj ka din is ok)= [.2,.6,.9] ,  where class1=.2,class2=.6,class=.9

Predict(tweet: aaj ka din is ok) = max(class1=.2,class2=.6,class3=.9)

d)Now, we can see that class3=.9 means 90% chances of Class3, 
tweet: aaj ka din is ok        our Model predict sentiment: Neutral

#data clean
import re
import csv
import os

class DataCleanerApp():

    def removeCommon(self,sentence):

        # remove punctuation
        # the unicode flag makes it work for more letter types (non-ascii)
        no_punc = re.sub(r'[^\w\s]', '', sentence, re.UNICODE)
        #print('No punctuation:', no_punc)

        # remove duplicates & stopwords. #regex format- [repeat words]|[Hin/Eng/http/https]|[special chars]
        re_output = re.sub(r'\b(\w+)( \1\b)+|(?<![\w\d])Hin|Eng|http|https(?![\w\d])|[â,Å,ðŸ,O,á,Ã,ě,ů,Ä,	]+|([A-Za-z]+[\d@]+[\w@]*|[\d@]+[A-Za-z]+[\w@]*)', r' ', no_punc)
        #print('No duplicates :', re_output)

        # remove any single letter
        re_output = ' '.join([w for w in re_output.split() if len(w) > 1])

        re_output = re_output.lower()

        return re_output

    def convertToTuple(self, cleanedSentence):
        first = cleanedSentence.split(' ',3)
        sentiment = str('NA')
        validSentiments = ['neutral','positive','negative']
        if first[2].strip().lower() in validSentiments :
            sentiment = first[2].strip().lower()

        return (first[3],first[1],sentiment)

    def convertToLines(self,path):
        f = open(path)
        lines = f.readlines()
        print("Raw Input : ")
        print(lines)

        lineTuples = [];
        tempLine = [];
        for line in lines:
            if line == '\n':
                oneLine = ''.join(tempLine).replace('\n', '\t')
                oneLine = self.removeCommon(oneLine)
                oneTuple = self.convertToTuple(oneLine)
                lineTuples.append(oneTuple)
                tempLine = [];
            else:
                tempLine.append(line)

        return lineTuples


    def writeCSV(self,data,dirPath):
      #  with open(dirPath+'\out.csv', 'w') as out:
         with open(dirPath, 'w') as out:
            csv_out = csv.writer(out)
            csv_out.writerow(['tweet', 'tweetid','sentiment'])
            csv_out.writerows(data)


    def worker(self):
        print('Running DataCleanerApp ...')
        path = input("Enter file path(Eg. - D:\File\\train.txt) : ")
        dirPath = '\\'.join(path.split('\\')[0:-1])
        print("Output will be written to path : "+dirPath)
        lineTuples = self.convertToLines(path);
        train_output  =dirPath + 'train.csv'
        self.writeCSV(lineTuples, train_output)
        print('Running DataCleanerApp  for Validation ...')
        path = input("Enter file path(Eg. - D:\File\\train.txt) : ")
        dirPath = '\\'.join(path.split('\\')[0:-1])
        print("Output will be written to path : "+dirPath)
        lineTuples = self.convertToLines(path);
        print(lineTuples)
        validation_output  =dirPath + 'Validation.csv'
        self.writeCSV(lineTuples, validation_output)
        print('Running DataCleanerApp  for test ...')
        path = input("Enter file path(Eg. - D:\File\\train.txt) : ")
        dirPath = '\\'.join(path.split('\\')[0:-1])
        print("Output will be written to path : "+dirPath)
        lineTuples = self.convertToLines(path);
        print(lineTuples)
        validation_output  =dirPath + 'test.csv'
        self.writeCSV(lineTuples, validation_output)
        print("Done !!")

#clean data in one go
# train data:        /content/train_14k_split_conll.txt,                o/p - train.csv
# validation  data:  /content/dev_3k_split_conll.txt,                   o/p - validation .csv
# test data       :  /content/Hindi_test_unalbelled_conll_updated.txt , o/p - test.csv
obj = DataCleanerApp()
obj.worker()

from sklearn.feature_extraction.text import TfidfVectorizer  #Training 
from sklearn.feature_selection import SelectKBest,chi2
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn import metrics

from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')

# filter function to check valid character
def filter_function(char):
  return char == '\n' or char == '\t' or 32<= ord(char) <=126

# preparing hindi & english stopList
#hindi stop words :  https://github.com/TrigonaMinima/HinglishNLP/blob/master/data/assets/stop_hinglish

stopList = [] 
f = open('/content/Hinglish_Stop.txt')
lines = f.readlines()
print("Raw Input : ")
print(lines)
for r  in lines:  #add hinglish stop words in Stoplist
  stopList.append(r)

# nltk  further clean up
from nltk.tokenize import wordpunct_tokenize

stopletter = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']

# prepare data from cleaned data we retrived from Dataclean
def further_clean(path):   
  l1 = []
  with open(path) as f:
    reader = csv.DictReader(f)
    for row in reader:
      l1.append(row)
  trainSets = l1[0:]
  X_train = []
  y_train =[]
  for val in trainSets:
    
    v= filter(filter_function,val['tweet'])
    v= val['tweet']
    list_of_word = [i for i in wordpunct_tokenize(v) if i not in stopletter]
    v= ' '.join(list_of_word)
    X_train.append(v)
    y_train.append(val['sentiment'])
  return X_train,y_train

#convert data to Tfidf format before model prepration 
def data_fitting(X_train,y_train):
  X_train = vectorizer.fit_transform(X_train)
  X_train  = ch2.fit_transform (X_train,y_train )
  return X_train,y_train

#predict function 
def predictfun(X_test,y_test,model):
  
  X_test = vectorizer.transform(X_test)
  X_test = ch2.transform(X_test)
  y_pred = model.predict(X_test)
  


  if y_test[1] == 'NA':
    print('This is TEST set!!! and our model has predicted ',y_pred[1:5])

  else:
    print('this is Validation set!!!!')
    print('Accuracy Score: ',metrics.accuracy_score(y_test,y_pred)*100,'%',sep='')
    print('Confusion Matrix: ',metrics.confusion_matrix(y_test,y_pred), sep = '\n')

# TfidfVectorizer,Chi2 for data transform
vectorizer = TfidfVectorizer(max_features=14000, ngram_range=(1,3),stop_words=stopList)
ch2 = SelectKBest(chi2, k='all')

#1. multinomial Naive bayes model

#prepare data 
path = '/content/train.csv'
X_train,y_train =further_clean(path)

#data fitting for model
X_train,y_train= data_fitting(X_train,y_train)

#model prepartion 
clf = MultinomialNB(alpha = .05)
clf.fit(X_train,y_train)

#2. SVM Model

#prepare data 
path = '/content/train.csv'
X_train,y_train =further_clean(path)

#data fitting for model
X_train,y_train= data_fitting(X_train,y_train)


#model prepartion 
SVM = LinearSVC(random_state=0, tol=1e-05)
SVM.fit(X_train, y_train)

print('\nSupport Vector Machine model SVM ready!!!')

#Predict for  Validation set
path = '/content/Validation.csv'
X_test_V,y_test_V =further_clean(path)

#X_test = vectorizer.transform(X_test)
#X_test = ch2.transform(X_test)

#predict for Validation set  model Multinomial
print('predict for Validation set  Model Multinomial !!!')
predictfun(X_test_V,y_test_V,clf)

#prepare data for Validation set
path = '/content/Validation.csv'
X_test_V,y_test_V =further_clean(path)

#predict for Validation set  model Multinomial
print('\n\n\n predict for Validation set  Model SVM !!!')
predictfun(X_test_V,y_test_V,SVM)

#Predict for  Test set
path = '/content/test.csv'
X_test,y_test =further_clean(path)

#predict for Validation set  model Multinomial
print('predict for TEST set  Model Multinomial !!!')
predictfun(X_test,y_test,clf)

# test set data prepration  for unlabbeled sentiment
path = '/content/test.csv'
X_test,y_test =further_clean(path)

#predict for Validation set  model Multinomial
print('\n\n\n predict for TEST set  Model SVM !!!')
predictfun(X_test,y_test,SVM)